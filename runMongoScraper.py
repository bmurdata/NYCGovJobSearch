# Main entry point for mongo scraping. No output files will be generated by default


import time
import argparse
import sys

from selCheck import Chrome_setup
import mongoScraperModule, monCheck
from monCheck import writeToMongo, writeToMongoMany
# from scrapeargs import args
from scrapeargs import linkSrchTemplate_Category, linkSrchTemplate_Code, agency_codes, careerInterest,jobLinkTemplate
import os 
from datetime import date
from multiprocessing import Pool
import traceback
start_time=time.time()  
agency_scrape_time="NA"
category_scrape_time="NA"
dir_path = os.path.dirname(os.path.realpath(__file__))+"/output/"

# Argparser
currTime=str(time.time()).split(".")[0]

default_code_file=str(date.today())+"_"+str(currTime)+"By-AgencyCode"

default_category_file=str(date.today())+"_"+str(currTime)+"By-Category"

parser = argparse.ArgumentParser(description="NYCGov Job site scraper. Outputs JSON and CSV files by job category and by specific agency.")

parser.add_argument("-afile","--agencyfile",help="Agency JSON and CSV file names.",
                    default=default_code_file)

parser.add_argument("-cfile","--categoryfile", help="Category JSON and CSV file names.",
                    default=default_category_file)
parser.add_argument("-writeDB","--writeDB",action="store_true",help="Update database from JSON data directly. Unless --noOutput is set, files will be created. Database must be setup for proper function.")
parser.add_argument("-noOutput","--noOutput",action="store_true",help="Stops creation of file output. Recommended only use when writeDB is set.")
parser.add_argument("-test","--test",action="store_true",help="Test script with two agencies to make sure all works.")

print(sys.argv[1:])
args = parser.parse_args()

# Run scraping and write to file 

print("Performing search scrape.")
if __name__== "__main__":
    print("Attempting to run a version with Mongo! Hoping for the best!"+str(currTime))
    code_jsonfile=dir_path+(args.agencyfile.split(".",1)[0]) + ".json"
    code_csvfile=dir_path+(args.agencyfile.split(".",1)[0]) +".csv"
    # run_scrape(code_jsonfile,agency_codes,linkSrchTemplate_Code,jobLinkTemplate,code_csvfile,args.writeDB,args.noOutput)
    badC=dict()
    if args.test==True:
        print("Running using test parameters")
        agency_codes={"ADMINISTRATION FOR CHILDRE": "067", "CUNY BRONX COMMUNITY COLLE": "463"}
    
    writeToMongo({"log":"Writing today 7-10-21"+str(currTime)},"writeLogs")
    try:
        monCheck.clearfromMongo('jobsByCode')
        myJobs=mongoScraperModule.jobScrape(agency_codes,badC,linkSrchTemplate_Code,jobLinkTemplate)
        writeToMongo({"log":"Writing this many jobs on 7-10-21: "+str(len(myJobs))+" at "+str(currTime)},"writeLogs")
        writeToMongoMany(myJobs,'jobsByCode')
        # for job in myJobs:
        #     writeToMongo(job,"jobsByCode")
    except Exception as e:
        writeToMongo({"log":"Writing today 7-10-21,"+str(currTime)+" something crashed"},"writeLogs")
        print("System failure dont know why. see below")
        print(e)
    if args.writeDB:
        print("Writing to DB")
        jobLinks=monCheck.mongoCompareAgencyandMeta_DB()
        mongoScraperModule.scrape_multi_arr(jobLinks)


# Scrape by Agency Code
# Location to put files
# code_jsonfile=dir_path+(args.agencyfile.split(".",1)[0]) + ".json"
# code_csvfile=dir_path+(args.agencyfile.split(".",1)[0]) +".csv"

# if args.test==True:
#     print("Running using test parameters")
#     agency_codes={"ADMINISTRATION FOR CHILDRE": "067", "CUNY BRONX COMMUNITY COLLE": "463"}
# run_scrape(code_jsonfile,agency_codes,linkSrchTemplate_Code,jobLinkTemplate,code_csvfile,args.writeDB,args.noOutput)

# agency_scrape_time=round(time.time()-start_time,2)
# print("------")
# print("Time to execute search by agency:{time}".format(time=agency_scrape_time))
# print("------")
# print("Time to execute search by category:{time}".format(time=category_scrape_time))

# final_time=round(time.time()-start_time,2)

# print("------")
# print("Final time to execute:{time}".format(time=final_time))
# print("To perform job scrape with multithreading run the following:")
# try:
#     input_code_json=code_jsonfile 
# except:
#     input_code_json= args.searchjsonfile
# print("python .\getJobDetails.py --joblinkfile "+ str(input_code_json))
# print("------")